{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Jacob/Desktop/prosjektoppgave/testing/gymming/src\n"
     ]
    }
   ],
   "source": [
    "%cd ../src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autograd for a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  tensor([0.0437, 0.7890, 0.4500], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "loss:  0.2756576226401392\n",
      "a.grad:  tensor([0.0291, 0.5260, 0.3000], dtype=torch.float64)\n",
      "b.grad:  tensor([0.0012, 0.0876, 0.0742], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# gradient descent on a\\na.data -= 0.1 * a.grad.data\\nprint('a: ', a)\\nloss = nn.functional.mse_loss(a, y)\\nprint('loss: ', loss.item())\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10, dtype=torch.float64)\n",
    "y = torch.zeros(3, dtype=torch.float64) # target tensor\n",
    "w = torch.randn(10, 3, requires_grad=True, dtype=torch.float64) # weights\n",
    "b = torch.randn(3, requires_grad=True, dtype=torch.float64) # bias\n",
    "z = x @ w + b\n",
    "a = torch.sigmoid(z)\n",
    "a.retain_grad()\n",
    "print('a: ', a)\n",
    "loss = nn.functional.mse_loss(a, y)\n",
    "print('loss: ', loss.item())\n",
    "loss.backward()\n",
    "print('a.grad: ', a.grad)\n",
    "print('b.grad: ', b.grad)\n",
    "'''\n",
    "# gradient descent on a\n",
    "a.data -= 0.1 * a.grad.data\n",
    "print('a: ', a)\n",
    "loss = nn.functional.mse_loss(a, y)\n",
    "print('loss: ', loss.item())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finite differences for b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite difference:  0.0012156942119645464\n",
      "finite difference:  0.08756217972916147\n",
      "finite difference:  0.0742483852178566\n"
     ]
    }
   ],
   "source": [
    "# estimate b gradients with finite differences\n",
    "with torch.no_grad():\n",
    "    h = 1e-10\n",
    "    b1 = b.clone()\n",
    "    for i in range(3):\n",
    "        b1[i] += h\n",
    "        z1 = x @ w + b1\n",
    "        a1 = torch.sigmoid(z1)\n",
    "        loss1 = nn.functional.mse_loss(a1, y)\n",
    "        print('finite difference: ', (loss1.item() - loss.item()) / h)\n",
    "        b1[i] -= h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finite differences for a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite difference:  0.029118929489868606\n",
      "finite difference:  0.526019783286813\n",
      "finite difference:  0.29999724926454974\n"
     ]
    }
   ],
   "source": [
    "# estimate a gradients with finite differences\n",
    "with torch.no_grad():\n",
    "    h = 1e-10\n",
    "    a1 = a.clone()\n",
    "    for i in range(3):\n",
    "        a1[i] += h\n",
    "        loss1 = nn.functional.mse_loss(a1, y)\n",
    "        print('finite difference: ', (loss1.item() - loss.item()) / h)\n",
    "        a1[i] -= h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autograd for cav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  tensor([0.9992, 0.1203, 0.3763], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "loss:  0.38480777941211314\n",
      "a.shape:  torch.Size([3])\n",
      "cav.shape:  torch.Size([3])\n",
      "a.grad:  tensor([0.6661, 0.0802, 0.2509], dtype=torch.float64)\n",
      "cav_grad:  tensor(0.2910, dtype=torch.float64)\n",
      "a1:  tensor([0.9992, 0.0476, 0.2308], dtype=torch.float64)\n",
      "a diff:  tensor([ 0.0000, -0.0728, -0.1455], dtype=torch.float64)\n",
      "loss1:  0.3512878126060858\n"
     ]
    }
   ],
   "source": [
    "cav = torch.tensor([0.0, 0.5, 1.0], dtype=torch.float64)\n",
    "\n",
    "x = torch.ones(10, dtype=torch.float64)\n",
    "y = torch.zeros(3, dtype=torch.float64) # target tensor\n",
    "w = torch.randn(10, 3, requires_grad=True, dtype=torch.float64) # weights\n",
    "b = torch.randn(3, requires_grad=True, dtype=torch.float64) # bias\n",
    "z = x @ w + b\n",
    "a = torch.sigmoid(z)\n",
    "a.retain_grad()\n",
    "print('a: ', a)\n",
    "loss = nn.functional.mse_loss(a, y)\n",
    "print('loss: ', loss.item())\n",
    "loss.backward()\n",
    "print('a.shape: ', a.shape)\n",
    "print('cav.shape: ', cav.shape)\n",
    "print('a.grad: ', a.grad)\n",
    "cav_grad = a.grad @ cav\n",
    "print('cav_grad: ', cav_grad)\n",
    "# update a with cav_grad\n",
    "with torch.no_grad():\n",
    "    a1 = a.clone()\n",
    "    a1 -= 0.5 * cav * cav_grad\n",
    "    print('a1: ', a1)\n",
    "    print('a diff: ', a1 - a)\n",
    "    loss1 = nn.functional.mse_loss(a1, y)\n",
    "    print('loss1: ', loss1.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finite differences for cav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perturb:  tensor([0.0000e+00, 5.0000e-11, 1.0000e-10], dtype=torch.float64)\n",
      "loss0:  0.38480777941211314\n",
      "loss1:  0.38480777944121325\n",
      "finite difference cav:  0.2910011120960121\n"
     ]
    }
   ],
   "source": [
    "# d L/d cav, when cav is a direction in a?\n",
    "# estimate cav gradients with finite differences\n",
    "with torch.no_grad():\n",
    "    h = 1e-10\n",
    "    a1 = a.clone()\n",
    "    perturb = h * cav\n",
    "    print('perturb: ', perturb)\n",
    "    a1 += perturb\n",
    "    loss1 = nn.functional.mse_loss(a1, y)\n",
    "    print('loss0: ', loss.item())\n",
    "    print('loss1: ', loss1.item())\n",
    "    print('finite difference cav: ', (loss1.item() - loss.item()) / h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
