{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from concepts import concept_instances\n",
    "from utils import prepare_folders\n",
    "from dqn import load_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autograd for a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  tensor([0.5702, 0.1111, 0.0018], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "loss:  0.11249074916824343\n",
      "a.grad:  tensor([0.3801, 0.0741, 0.0012], dtype=torch.float64)\n",
      "b.grad:  tensor([9.3160e-02, 7.3130e-03, 2.0852e-06], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# gradient descent on a\\na.data -= 0.1 * a.grad.data\\nprint('a: ', a)\\nloss = nn.functional.mse_loss(a, y)\\nprint('loss: ', loss.item())\\n\""
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10, dtype=torch.float64)\n",
    "y = torch.zeros(3, dtype=torch.float64) # target tensor\n",
    "w = torch.randn(10, 3, requires_grad=True, dtype=torch.float64) # weights\n",
    "b = torch.randn(3, requires_grad=True, dtype=torch.float64) # bias\n",
    "z = x @ w + b\n",
    "a = torch.sigmoid(z)\n",
    "a.retain_grad()\n",
    "print('a: ', a)\n",
    "loss = nn.functional.mse_loss(a, y)\n",
    "print('loss: ', loss.item())\n",
    "loss.backward()\n",
    "print('a.grad: ', a.grad)\n",
    "print('b.grad: ', b.grad)\n",
    "'''\n",
    "# gradient descent on a\n",
    "a.data -= 0.1 * a.grad.data\n",
    "print('a: ', a)\n",
    "loss = nn.functional.mse_loss(a, y)\n",
    "print('loss: ', loss.item())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finite differences for b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite difference:  0.09316020177507767\n",
      "finite difference:  0.007313039063205906\n",
      "finite difference:  2.0816681711721685e-06\n"
     ]
    }
   ],
   "source": [
    "# estimate b gradients with finite differences\n",
    "with torch.no_grad():\n",
    "    h = 1e-10\n",
    "    b1 = b.clone()\n",
    "    for i in range(3):\n",
    "        b1[i] += h\n",
    "        z1 = x @ w + b1\n",
    "        a1 = torch.sigmoid(z1)\n",
    "        loss1 = nn.functional.mse_loss(a1, y)\n",
    "        print('finite difference: ', (loss1.item() - loss.item()) / h)\n",
    "        b1[i] -= h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finite differences for a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finite difference:  0.380133979849262\n",
      "finite difference:  0.07405798196913338\n",
      "finite difference:  0.0011801670751765414\n"
     ]
    }
   ],
   "source": [
    "# estimate a gradients with finite differences\n",
    "with torch.no_grad():\n",
    "    h = 1e-10\n",
    "    a1 = a.clone()\n",
    "    for i in range(3):\n",
    "        a1[i] += h\n",
    "        loss1 = nn.functional.mse_loss(a1, y)\n",
    "        print('finite difference: ', (loss1.item() - loss.item()) / h)\n",
    "        a1[i] -= h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autograd for cav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:  tensor([0.1506, 0.7710, 0.0926], dtype=torch.float64,\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "loss:  0.2085440093811767\n",
      "a.shape:  torch.Size([3])\n",
      "cav.shape:  torch.Size([3])\n",
      "a.grad:  tensor([0.1004, 0.5140, 0.0617], dtype=torch.float64)\n",
      "cav_grad:  tensor(0.3187, dtype=torch.float64)\n",
      "a1:  tensor([ 0.1506,  0.6913, -0.0668], dtype=torch.float64)\n",
      "a diff:  tensor([ 0.0000, -0.0797, -0.1594], dtype=torch.float64)\n",
      "loss1:  0.16833698155411003\n"
     ]
    }
   ],
   "source": [
    "cav = torch.tensor([0.0, 0.5, 1.0], dtype=torch.float64)\n",
    "\n",
    "x = torch.ones(10, dtype=torch.float64)\n",
    "y = torch.zeros(3, dtype=torch.float64) # target tensor\n",
    "w = torch.randn(10, 3, requires_grad=True, dtype=torch.float64) # weights\n",
    "b = torch.randn(3, requires_grad=True, dtype=torch.float64) # bias\n",
    "z = x @ w + b\n",
    "a = torch.sigmoid(z)\n",
    "a.retain_grad()\n",
    "print('a: ', a)\n",
    "loss = nn.functional.mse_loss(a, y)\n",
    "print('loss: ', loss.item())\n",
    "loss.backward()\n",
    "print('a.shape: ', a.shape)\n",
    "print('cav.shape: ', cav.shape)\n",
    "print('a.grad: ', a.grad)\n",
    "cav_grad = a.grad @ cav\n",
    "print('cav_grad: ', cav_grad)\n",
    "# update a with cav_grad\n",
    "with torch.no_grad():\n",
    "    a1 = a.clone()\n",
    "    a1 -= 0.5 * cav * cav_grad\n",
    "    print('a1: ', a1)\n",
    "    print('a diff: ', a1 - a)\n",
    "    loss1 = nn.functional.mse_loss(a1, y)\n",
    "    print('loss1: ', loss1.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finite differences for cav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perturb:  tensor([0.0000e+00, 5.0000e-11, 1.0000e-10], dtype=torch.float64)\n",
      "loss0:  0.2980911451220287\n",
      "loss1:  0.29809114518388335\n",
      "finite difference ca:  0.6185463252705858\n"
     ]
    }
   ],
   "source": [
    "# d L/d cav, when cav is a direction in a?\n",
    "# estimate cav gradients with finite differences\n",
    "with torch.no_grad():\n",
    "    h = 1e-10\n",
    "    a1 = a.clone()\n",
    "    perturb = h * cav\n",
    "    print('perturb: ', perturb)\n",
    "    a1 += perturb\n",
    "    loss1 = nn.functional.mse_loss(a1, y)\n",
    "    print('loss0: ', loss.item())\n",
    "    print('loss1: ', loss1.item())\n",
    "    print('finite difference cav: ', (loss1.item() - loss.item()) / h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
